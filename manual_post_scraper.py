#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í¼ - ì‚¬ìš©ìê°€ ì§ì ‘ ê²Œì‹œë¬¼ì„ í´ë¦­í•˜ë„ë¡ ì•ˆë‚´
"""
import asyncio
import json
from datetime import datetime
from loguru import logger
from scrapers.base_scraper import BaseScraper
from config.settings import settings

class ManualPostScraper(BaseScraper):
    """ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í¼"""
    
    def __init__(self, db_service=None, performance_tracker=None, config=None):
        super().__init__(db_service, performance_tracker, config)
        
        # ê²Œì‹œë¬¼ ê´€ë ¨ ì„ íƒìë“¤
        self.selectors = {
            "like_count": "[data-e2e='like-count']",
            "comment_count": "[data-e2e='comment-count']",
            "share_count": "[data-e2e='share-count']",
            "play_count": "[data-e2e='play-count']",
            "post_content": "[data-e2e='user-post-item-desc']",
            "comment_container": "[data-e2e='comment-level-1']",
            "comment_text": "[data-e2e='comment-level-1'] [data-e2e='comment-text']",
            "comment_author": "[data-e2e='comment-level-1'] [data-e2e='comment-username']",
            "comment_likes": "[data-e2e='comment-level-1'] [data-e2e='comment-like-count']"
        }
    
    async def scrape_profile_with_manual_help(self, influencer_id: str) -> dict:
        """ì‚¬ìš©ì ë„ì›€ì„ ë°›ì•„ í”„ë¡œí•„ê³¼ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘"""
        try:
            logger.info(f"ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {influencer_id}")
            
            # TikTok í”„ë¡œí•„ URL
            profile_url = f"{self.config.tiktok.base_url}/@{influencer_id}"
            
            # í˜ì´ì§€ ì´ë™
            if not await self._navigate_to_page(profile_url):
                logger.error(f"í”„ë¡œí•„ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {influencer_id}")
                return {}
            
            # ì½˜í…ì¸  ë¡œë“œ ëŒ€ê¸°
            await self._wait_for_content_load()
            
            # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´
            print("\n" + "="*60)
            print("ğŸ¯ ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ëª¨ë“œ")
            print("="*60)
            print(f"í˜„ì¬ í˜ì´ì§€: @{influencer_id}")
            print("\nğŸ“‹ ì§€ì‹œì‚¬í•­:")
            print("1. ë¸Œë¼ìš°ì €ì—ì„œ ê²Œì‹œë¬¼ì„ í´ë¦­í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì„¸ìš”")
            print("2. ê²Œì‹œë¬¼ì´ ë¡œë“œë˜ë©´ Enterë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”")
            print("3. ëŒ“ê¸€ì„ ë” ë³´ë ¤ë©´ ìŠ¤í¬ë¡¤í•˜ì„¸ìš”")
            print("4. ë‹¤ìŒ ê²Œì‹œë¬¼ë¡œ ì´ë™í•˜ë ¤ë©´ 'next'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            print("5. ì¢…ë£Œí•˜ë ¤ë©´ 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
            print("="*60)
            
            posts_data = []
            post_index = 0
            
            while True:
                try:
                    # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
                    user_input = input(f"\nê²Œì‹œë¬¼ {post_index + 1} ì¤€ë¹„ ì™„ë£Œ? (Enter/next/quit): ").strip().lower()
                    
                    if user_input == 'quit':
                        break
                    elif user_input == 'next':
                        print("ë‹¤ìŒ ê²Œì‹œë¬¼ë¡œ ì´ë™í•˜ì„¸ìš”...")
                        continue
                    
                    # í˜„ì¬ í˜ì´ì§€ URL í™•ì¸
                    current_url = self.page.url
                    logger.info(f"í˜„ì¬ URL: {current_url}")
                    
                    # ê²Œì‹œë¬¼ í˜ì´ì§€ì¸ì§€ í™•ì¸
                    if '/video/' not in current_url:
                        print("âŒ ê²Œì‹œë¬¼ í˜ì´ì§€ê°€ ì•„ë‹™ë‹ˆë‹¤. ê²Œì‹œë¬¼ì„ í´ë¦­í•´ì£¼ì„¸ìš”.")
                        continue
                    
                    # ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ
                    post_data = await self._extract_current_post_data(influencer_id, post_index, current_url)
                    
                    if post_data:
                        posts_data.append(post_data)
                        print(f"âœ… ê²Œì‹œë¬¼ {post_index + 1} ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
                        print(f"   ì¢‹ì•„ìš”: {post_data.get('hearts', 0):,}")
                        print(f"   ëŒ“ê¸€: {post_data.get('comment_count', 0):,}")
                        print(f"   ê³µìœ : {post_data.get('shares', 0):,}")
                        print(f"   ì¡°íšŒìˆ˜: {post_data.get('views', 0):,}")
                        print(f"   ëŒ“ê¸€ ìˆ˜ì§‘: {len(post_data.get('comments', []))}ê°œ")
                        
                        post_index += 1
                    else:
                        print("âŒ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨")
                    
                except KeyboardInterrupt:
                    print("\n\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    break
                except Exception as e:
                    logger.error(f"ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
                    print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            result = {
                "influencer_id": influencer_id,
                "profile_url": profile_url,
                "posts": posts_data,
                "total_posts": len(posts_data),
                "scraped_at": datetime.now().isoformat()
            }
            
            logger.info(f"ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {influencer_id} - {len(posts_data)}ê°œ")
            return result
            
        except Exception as e:
            logger.error(f"ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {influencer_id} - {e}")
            return {}
    
    async def _extract_current_post_data(self, influencer_id: str, index: int, post_url: str) -> dict:
        """í˜„ì¬ í˜ì´ì§€ì˜ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # ê²Œì‹œë¬¼ ë°ì´í„° ì´ˆê¸°í™”
            post_data = {
                "influencer_id": influencer_id,
                "post_url": post_url,
                "post_index": index,
                "scraped_at": datetime.now().isoformat()
            }
            
            # ê²Œì‹œë¬¼ ID ì¶”ì¶œ
            post_id = self._extract_post_id(post_url)
            post_data["post_id"] = post_id
            
            # ê²Œì‹œë¬¼ í†µê³„ ì •ë³´ ì¶”ì¶œ
            await self._extract_post_statistics(post_data)
            
            # ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ
            await self._extract_post_content(post_data)
            
            # ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ
            comments = await self._extract_comments(post_data)
            post_data["comments"] = comments
            post_data["comment_count"] = len(comments)
            
            return post_data
            
        except Exception as e:
            logger.error(f"í˜„ì¬ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _extract_post_id(self, post_url: str) -> str:
        """ê²Œì‹œë¬¼ ID ì¶”ì¶œ"""
        try:
            import re
            # URLì—ì„œ ê²Œì‹œë¬¼ ID ì¶”ì¶œ
            match = re.search(r'/video/(\d+)', post_url)
            if match:
                return match.group(1)
            
            # ëŒ€ì²´ ë°©ë²•
            parts = post_url.split('/')
            for part in parts:
                if part.isdigit() and len(part) > 10:
                    return part
            
            return "unknown"
            
        except Exception as e:
            logger.error(f"ê²Œì‹œë¬¼ ID ì¶”ì¶œ ì‹¤íŒ¨: {post_url} - {e}")
            return "unknown"
    
    async def _extract_post_statistics(self, post_data: dict):
        """ê²Œì‹œë¬¼ í†µê³„ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì¢‹ì•„ìš” ìˆ˜
            likes_text = await self._get_text_content(self.selectors["like_count"])
            post_data["hearts"] = self._parse_count(likes_text)
            
            # ëŒ“ê¸€ ìˆ˜
            comments_text = await self._get_text_content(self.selectors["comment_count"])
            post_data["comment_count_raw"] = self._parse_count(comments_text)
            
            # ê³µìœ  ìˆ˜
            shares_text = await self._get_text_content(self.selectors["share_count"])
            post_data["shares"] = self._parse_count(shares_text)
            
            # ì¡°íšŒìˆ˜
            views_text = await self._get_text_content(self.selectors["play_count"])
            post_data["views"] = self._parse_count(views_text)
            
        except Exception as e:
            logger.error(f"ê²Œì‹œë¬¼ í†µê³„ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    async def _extract_post_content(self, post_data: dict):
        """ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ"""
        try:
            # ê²Œì‹œë¬¼ ë‚´ìš©
            content = await self._get_text_content(self.selectors["post_content"])
            post_data["content"] = content.strip() if content else ""
            
            # í•´ì‹œíƒœê·¸ ì¶”ì¶œ
            import re
            hashtags = re.findall(r'#\w+', post_data.get("content", ""))
            post_data["hashtags"] = hashtags
            
            # ë©˜ì…˜ ì¶”ì¶œ
            mentions = re.findall(r'@\w+', post_data.get("content", ""))
            post_data["mentions"] = mentions
            
        except Exception as e:
            logger.error(f"ê²Œì‹œë¬¼ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    
    async def _extract_comments(self, post_data: dict) -> list:
        """ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info("ëŒ“ê¸€ ì¶”ì¶œ ì‹œì‘...")
            
            # ëŒ“ê¸€ ì„¹ì…˜ì´ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            try:
                await self.page.wait_for_selector(self.selectors["comment_container"], timeout=10000)
            except:
                logger.warning("ëŒ“ê¸€ ì„¹ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            # ëŒ“ê¸€ ë” ë¡œë“œ (ìŠ¤í¬ë¡¤)
            await self._load_more_comments()
            
            # ëŒ“ê¸€ ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸°
            comment_containers = await self.page.query_selector_all(self.selectors["comment_container"])
            
            if not comment_containers:
                logger.warning("ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            logger.info(f"ë°œê²¬ëœ ëŒ“ê¸€: {len(comment_containers)}ê°œ")
            
            comments = []
            for i, container in enumerate(comment_containers[:50]):  # ìµœëŒ€ 50ê°œ ëŒ“ê¸€
                try:
                    comment_data = await self._extract_single_comment(container, i)
                    if comment_data:
                        comments.append(comment_data)
                except Exception as e:
                    logger.debug(f"ëŒ“ê¸€ {i} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"ëŒ“ê¸€ ì¶”ì¶œ ì™„ë£Œ: {len(comments)}ê°œ")
            return comments
            
        except Exception as e:
            logger.error(f"ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _load_more_comments(self):
        """ë” ë§ì€ ëŒ“ê¸€ ë¡œë“œ"""
        try:
            # ëŒ“ê¸€ ì„¹ì…˜ ìŠ¤í¬ë¡¤
            await self.page.evaluate("""
                () => {
                    const commentSection = document.querySelector('[data-e2e="comment-level-1"]');
                    if (commentSection) {
                        commentSection.scrollIntoView();
                    }
                }
            """)
            
            await self._random_delay(1, 2)
            
            # ì¶”ê°€ ìŠ¤í¬ë¡¤
            for _ in range(3):
                await self.page.evaluate("window.scrollBy(0, 500)")
                await self._random_delay(1, 2)
                
        except Exception as e:
            logger.debug(f"ëŒ“ê¸€ ë” ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    async def _extract_single_comment(self, container, index: int) -> dict:
        """ë‹¨ì¼ ëŒ“ê¸€ ì •ë³´ ì¶”ì¶œ"""
        try:
            comment_data = {
                "comment_index": index,
                "scraped_at": datetime.now().isoformat()
            }
            
            # ëŒ“ê¸€ í…ìŠ¤íŠ¸
            text_element = await container.query_selector(self.selectors["comment_text"])
            if text_element:
                text = await text_element.text_content()
                comment_data["text"] = text.strip() if text else ""
            
            # ëŒ“ê¸€ ì‘ì„±ì
            author_element = await container.query_selector(self.selectors["comment_author"])
            if author_element:
                author = await author_element.text_content()
                comment_data["author"] = author.strip() if author else ""
            
            # ëŒ“ê¸€ ì¢‹ì•„ìš” ìˆ˜
            likes_element = await container.query_selector(self.selectors["comment_likes"])
            if likes_element:
                likes_text = await likes_element.text_content()
                comment_data["likes"] = self._parse_count(likes_text)
            else:
                comment_data["likes"] = 0
            
            # ëŒ“ê¸€ ID ìƒì„±
            comment_data["comment_id"] = f"{comment_data.get('author', 'unknown')}_{index}"
            
            return comment_data
            
        except Exception as e:
            logger.debug(f"ë‹¨ì¼ ëŒ“ê¸€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}
    
    def _parse_count(self, text: str) -> int:
        """ìˆ«ì í…ìŠ¤íŠ¸ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜"""
        if not text:
            return 0
        
        try:
            import re
            # ì‰¼í‘œ ì œê±°
            text = text.replace(',', '')
            
            # K, M, B ë‹¨ìœ„ ì²˜ë¦¬
            text_lower = text.lower().strip()
            if 'k' in text_lower:
                number = float(re.findall(r'[\d.]+', text_lower)[0])
                return int(number * 1000)
            elif 'm' in text_lower:
                number = float(re.findall(r'[\d.]+', text_lower)[0])
                return int(number * 1000000)
            elif 'b' in text_lower:
                number = float(re.findall(r'[\d.]+', text_lower)[0])
                return int(number * 1000000000)
            
            # ì¼ë°˜ ìˆ«ì
            numbers = re.findall(r'[\d]+', text)
            if numbers:
                return int(numbers[0])
            
            return 0
            
        except Exception as e:
            logger.error(f"ìˆ«ì íŒŒì‹± ì‹¤íŒ¨: {text} - {e}")
            return 0

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    scraper = None
    try:
        # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
        scraper = ManualPostScraper(config=settings)
        await scraper.initialize()
        
        # í…ŒìŠ¤íŠ¸í•  ì¸í”Œë£¨ì–¸ì„œ
        test_influencer = "charlidamelio"
        
        print(f"\n{'='*60}")
        print("ğŸ¯ TikTok ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í¼")
        print("="*60)
        print(f"ëŒ€ìƒ: @{test_influencer}")
        print("ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©´ ê²Œì‹œë¬¼ì„ í´ë¦­í•˜ì—¬ ìƒì„¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì„¸ìš”.")
        print("="*60)
        
        # ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘
        result = await scraper.scrape_profile_with_manual_help(test_influencer)
        
        if result and result.get("posts"):
            # ê²°ê³¼ ì €ì¥
            with open("manual_post_results.json", "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            
            print(f"\n{'='*60}")
            print("ğŸ‰ ìˆ˜ë™ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ!")
            print("="*60)
            print(f"ìˆ˜ì§‘ëœ ê²Œì‹œë¬¼: {len(result['posts'])}ê°œ")
            print("ê²°ê³¼ê°€ manual_post_results.json íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            # ìš”ì•½ ì¶œë ¥
            for i, post in enumerate(result["posts"], 1):
                print(f"\nê²Œì‹œë¬¼ {i}:")
                print(f"  ì¢‹ì•„ìš”: {post.get('hearts', 0):,}")
                print(f"  ëŒ“ê¸€: {post.get('comment_count', 0):,}")
                print(f"  ê³µìœ : {post.get('shares', 0):,}")
                print(f"  ì¡°íšŒìˆ˜: {post.get('views', 0):,}")
                print(f"  ëŒ“ê¸€ ìˆ˜ì§‘: {len(post.get('comments', []))}ê°œ")
        else:
            print("\nâŒ ê²Œì‹œë¬¼ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨")
            
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
    finally:
        if scraper:
            await scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(main()) 